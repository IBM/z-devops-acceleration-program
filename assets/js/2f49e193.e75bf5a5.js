"use strict";(self.webpackChunkdap_docs=self.webpackChunkdap_docs||[]).push([[2044],{8480:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"2024-03-08-cloning-large-z-repos","metadata":{"permalink":"/z-devops-acceleration-program/news/2024-03-08-cloning-large-z-repos","source":"@site/news/2024-03-08-repo-size-and-speed.md","title":"Worried about cloning your large Z code Git repo?","description":"TL;DR Don\'t worry about the size of your repos, they will be fine.","date":"2024-03-08T00:00:00.000Z","formattedDate":"March 8, 2024","tags":[{"label":"Git","permalink":"/z-devops-acceleration-program/news/tags/git"}],"hasTruncateMarker":false,"authors":[{"name":"Ian J Mitchell"}],"frontMatter":{"title":"Worried about cloning your large Z code Git repo?","slug":"2024-03-08-cloning-large-z-repos","authors":[{"name":"Ian J Mitchell"}],"tags":["Git"],"hide_table_of_contents":false,"hide_reading_time":true},"unlisted":false,"nextItem":{"title":"A standardized DevOps pipeline at RBC improves the developer experience and drives global business value","permalink":"/z-devops-acceleration-program/news/2023-12-27-transform-rbc"}},"content":"**TL;DR** Don\'t worry about the size of your repos, they will be fine.\\n\\n----\\n\\nI\'ve seen and heard a few mainframe development teams new to Git be fairly shocked\\nwhen they realise what the word \'distributed\' means when we say it is \\na \'distributed version control system\'. [read more...](https://community.ibm.com/community/user/ibmz-and-linuxone/blogs/ian-mitchell1/2024/03/06/worried-about-cloning-your-large-z-code-repo?CommunityKey=f461c55d-159c-4a94-b708-9f7fe11d972b)\\n\\n\\n\x3c!-- \\nLEAVING THE ORIGINAL TEXT HERE AS A RECORD\\n\\nThat term is on the cover page for \\nthe [Pro Git](https://git-scm.com/) book:\\n\\n> Git is a free and open source distributed version control system designed to handle everything from small to very large projects with speed and efficiency. \\n\\nTo a mainframe person, perhaps led by the way IBMers have historically talked,\\n\'distributed\' can be a catch-all term simply meaning \'non-mainframe\' - that\'s **not** what we mean here. Git is \\n\'distributed\' in a way **inclusive** of Z and z/OS.\\n\\nAgain quoting from [Pro Git](https://git-scm.com/about/distributed), Git\'s distributed nature is described as:\\n\\n>One of the nicest features of any Distributed SCM, Git included, is that it\'s distributed. This means that instead of doing a \\"checkout\\" of the current tip of the source code, you do a \\"clone\\" of the entire repository. \\n\\n*\\"clone the entire repository\\"* does often take a developer\\nused to all their code being managed in a traditional z/OS \\nlibrary manager as a huge surprise and adjustment.\\n\\nCommon concerns expressed at this point include:\\n\\n- Why would I want to do that?\\n- How can that possibly work in a team with everyone doing it?\\n- Our code base is too large for that to work.\\n- Cloning all of that all the time is going to be horrendously slow.\\n- Security and governance won\'t allow us to take our code off z/OS.\\n\\nLet\'s look at each of them.\\n\\n## Why would I want to clone the entire repository?\\n\\nAnother tagline in Pro Git is \\"distributed even if your workflow isn\'t\\" - this is particularly \\nrelevant to most Z scenarios when using Git. Most projects hosted on a git-based service, even\\nthe most open of open source ones, have a \'primary\' repo on a well-known server - the one on \\ngithub.com or gitlab.com for example. That is the control point for the project. You could say\\nthe project is centralized on that copy of the repository. It\'s that copy which will \\nhave all the governance and provenance related to the project\'s contents, and from where \\nconsumers will get source and/or builds.\\n\\nBut contributing developers to the project will clone it - with hundreds or thousands of \\ncontributors (or just people interested enough to look at it in sufficient detail), each\\nperson will make one or more clones - and that\'s fine (all subject to the appropriate\\nopen source licensing, of course).\\n\\nIn our corporate world the pattern will be the same, but entirely within your enterprise.\\nThe source will not be under an open source license, and developers will need to be authorized\\nto access the \'primary\' or \'central\' instance of the repo. Any change to the content will\\nhave to make it to this instance of the repo to stand a chance of making it to production\\nsince only that repo is plugged into the build, test and deployment systems which feed\\nthe production systems - you don\'t build or deploy from anywhere else.\\n\\nBut they will work with clones in\\nthe same way, using the same workflows as developers in the open source world. This is \\none of the reasons adopting a git-based SCM solves some of the skills challenges compared\\nto sticking with other tools.\\n\\nIn both the open source and corporate worlds, this is what git was built to do.\\n\\nYou could regard developers\' clones as mere local optimisations which decouple them from\\nthe central repo and the server on which it resides. But it\'s also nice that with all these\\ncopies, there are plenty of backups if disaster strikes the server of your central repo.\\n\\nFor the developer, having the clone means they are able to work with that copy and be \\nindependent of constant connectivity to a central server. Obviously they are not 100%\\nindependent since they should regularly synchronize, but it\'s their choice when to do\\nthat - it would typically be multiple times in a working day, but not every few seconds.\\n\\nGit has made this synchronization largely frictionless and natural to a developer\'s workflow.\\n\\nWhat is perhaps unfamiliar here? By default, the developer clones provide free-rein for \\ndevelopers to browse and edit any file in the repo and commit those changes. Edits\\ncan be changes to existing files, creation of new files, movement of code between files,\\nor renaming of files. They also have freedom to make new branches to work on multiple\\nwork items concurrently within the clone.\\n\\nThe frictionless synchronization publishes the work of each developer in their clone\\nto the central copy (this is not *merging*) on a branch-by-branch basis. The synchronization\\nis bi-directional.\\n\\n## How can that possibly work in a team with everyone doing it?\\n\\nLooking at the advantages of cloning from a central repo instance and having frictionless\\nsynchronization you might think that\'s great for individual developers or even small\\nteams, but how can it scale to larger teams?\\n\\nThis is where the importance of a good, scalable branching workflow comes in.\\n\\nIn my clone, I can create as many branches as I like to cover different pieces of work. \\nIt\'s important to use a good naming scheme for branches to keep track of them.\\n\\nYou can choose if and when to publish any branches to the server copy of the repo - so that\\nother developers can at least see what you\'re working on. Similarly, you can see\\nwhat other developers are working on.\\n\\nBut git is very happy with lots of branches - branches are very cheap, both quick\\nto create and space-efficient.\\n\\nProbably the most important thing is the naming (like most aspects of software engineering!).\\n\\n## Our code base is too large for that to work.\\n\\nHow big is your code base, really? There are some really big open source code bases hosted\\nin the various git service providers.\\n\\nPerhaps the largest reported repo, albeit not in an open source project is \\nthe Windows operating system. In [this blog post](https://blog.gitbutler.com/git-tips-3-really-large-repositories/)\\nScott Chacon says:\\n\\n> In 2017, Microsoft successfully moved the Windows codebase to Git. Brian Harry wrote a really great blog post about it called [The Largest Git Repo on the Planet](https://devblogs.microsoft.com/bharry/the-largest-git-repo-on-the-planet/) that you should read if you\'re interested, but the size and scope of this repository is astounding.\\n> \\n> - 3.5M files\\n>   - for reference, the Linux kernel is about 80k files, or 2% of that\\n> - 300GB repository (vs ~4.5G Linux kernel)\\n> - 4,000 active developers\\n> - 8,421 pushes per day (on average)\\n> - 4,352 active topic branches\\n\\nSince that time Microsoft and other contributors to the core Git project have worked very hard to\\nmake working with large repos more efficient.\\n\\nWith educated use of the specific features created to allow effective use of such large repos,\\npure size should probably NOT be a primary concern. Note that I said \'*educated use*\' - as with\\nmost things it takes some thinking, judgement and practice to get right.\\n\\nThere are at least three obvious scenarios where these specific features need to be employed:\\n\\n- the server\'s copy of the repo - the one(s) held on your chosen Git service provider\'s server.\\n  - among the examples of the effort put into this is optimization of garbage collection. See [Improving large monorepo performance on GitHub](https://github.blog/2021-03-16-improving-large-monorepo-performance-on-github/) and [Scaling monorepo maintenance](https://github.blog/2021-04-29-scaling-monorepo-maintenance/).\\n- a developer\'s clone where they work day-to-day.\\n  - a good option here is to employ partial cloning so the developer\'s clone has only the folders \\n    for the components they are working on. See [Bring your monorepo down to size with sparse-checkout](https://github.blog/2020-01-17-bring-your-monorepo-down-to-size-with-sparse-checkout/).\\n- a build system\'s clone.\\n  - a build system would typically not require the entire history so a shallow clone of limited depth\\n    should be all that\'s needed. See [Get up to speed with partial clone and shallow clone](https://github.blog/2020-12-21-get-up-to-speed-with-partial-clone-and-shallow-clone/).\\n\\n## Cloning all of that all the time is going to be horrendously slow.\\n\\nOne of the more common questions I\'ve encountered is simply the time a developer would spend \\nwaiting for their machine to clone the repo from the server.\\n\\nThere are, of course, two aspects to this:\\n\\n- How often does a developer need to clone the repo?\\n- Is cloning really slow in practice?\\n\\nThe first one will be determined by working practices, but developers working frequently with\\nthe same set of repos would not NEED to clone very often - once an initial clone is created it\\nwill happily live on their filesystem and be maintained with regular (probably daily) `pull`s.\\n(`Pull` speed is more determined by the rate of change of the server copy - so the more infrequently\\nthe synchronization is done, the more likely it is that the `pull` is slow and complex.)\\n\\nBut what about the second question - what is cloning like in practice?\\n\\nThere will be three factors to consider:\\n\\n- Amount of data to be transferred over the network between the server copy and the developer\'s machine.\\n- Speed of the network.\\n- Speed of the developer\'s machine - Git employs a lot of data compression to its data both at rest and \\n  for transmission, but it will need some CPU to do this. \\n\\nThese are going to be too variable to make any guarantees of outcomes. When faced with this question,\\nI explain these factors but the most effective answer is \\"try it and see\\". Luckily with so many open\\nsource repos on the various git service providers, it\'s easy to do some experiments.\\n\\nI\'ve looked at 3 repos - Kubernetes(K8S), Gitlab and Inkscape.\\n\\nHere\'s the results for a default full `clone`...\\n\\n### Kubernetes\\n\\n```\\n\u276f time git clone https://github.com/kubernetes/kubernetes.git\\nCloning into \'kubernetes\'...\\nremote: Enumerating objects: 1492376, done.\\nremote: Counting objects: 100% (95/95), done.\\nremote: Compressing objects: 100% (41/41), done.\\nremote: Total 1492376 (delta 57), reused 54 (delta 54), pack-reused 1492281\\nReceiving objects: 100% (1492376/1492376), 992.62 MiB | 3.14 MiB/s, done.\\nResolving deltas: 100% (1092167/1092167), done.\\nUpdating files: 100% (24944/24944), done.\\ngit clone https://github.com/kubernetes/kubernetes.git  128.82s user 52.97s system 52% cpu 5:43.32 total\\n\u276f du -sh kubernetes\\n1.3G\\tkubernetes\\n```\\n\\n### Gitlab source repo on Gitlab\\n\\n```\\n\u276f time git clone https://gitlab.com/gitlab-org/gitlab.git\\nCloning into \'gitlab\'...\\nremote: Enumerating objects: 5065256, done.\\nremote: Counting objects: 100% (113759/113759), done.\\nremote: Compressing objects: 100% (20020/20020), done.\\nremote: Total 5065256 (delta 103489), reused 100503 (delta 93107), pack-reused 4951497\\nReceiving objects: 100% (5065256/5065256), 1.94 GiB | 2.57 MiB/s, done.\\nResolving deltas: 100% (3964203/3964203), done.\\nUpdating files: 100% (69447/69447), done.\\ngit clone https://gitlab.com/gitlab-org/gitlab.git  279.46s user 99.49s system 43% cpu 14:28.46 total\\n```\\n### Inkscape on Gitlab\\n\\n```\\n\u276f time git clone https://gitlab.com/inkscape/inkscape.git\\nCloning into \'inkscape\'...\\nremote: Enumerating objects: 346436, done.\\nremote: Counting objects: 100% (2433/2433), done.\\nremote: Compressing objects: 100% (538/538), done.\\nremote: Total 346436 (delta 1973), reused 2253 (delta 1895), pack-reused 344003\\nReceiving objects: 100% (346436/346436), 1.72 GiB | 2.99 MiB/s, done.\\nResolving deltas: 100% (282385/282385), done.\\nUpdating files: 100% (5876/5876), done.\\ngit clone https://gitlab.com/inkscape/inkscape.git  122.20s user 58.67s system 29% cpu 10:08.82 total\\n```\\n\\nThis was over my home broadband connection\\n(which is nothing fancy, the router is currently reporting `Downstream sync speed: 32.232 Mbps, Upstream sync speed: 5.871 Mbps`). \\nYou can see the overall download rates were quite consistent so likely determined by that network bandwidth. \\nThese tests ran on a 2021 Macbook Pro with the M1 Pro CPU and 16GB RAM.\\n\\n|              | K8S | Gitlab | Inkscape |\\n|--------------|-----|--------|----------|\\n| Host         |  github.com | gitlab.com | gitlab.com |\\n| Xfer size GB | 0.992 | 1.94   | 1.72  |\\n| Disk size GB | 1.3   | 2.6    | 2.1   |\\n| Clone time   | 5m43s | 14m28s | 10m8s |\\n| no. of files | 24995 | 69447  | 5907  |\\n| commits      | 120K  | 370K   | 27.7K |\\n\\n### Minimal clone using `--depth 1` \\n\\n`--depth 1` gives you a full set of files but with none of the history before the most recent commit \\nto illustrate how dramatically clone options can affect the time and space requirements.\\n\\n*(note, sizes now in MB!)*\\n\\n|              | K8S | Gitlab | Inkscape |\\n|--------------|-----|--------|----------|\\n| Host         |  github.com | gitlab.com | gitlab.com |\\n| Xfer size MB | 42.5 | 120   | 57   |\\n| Disk size MB | 381  | 683   | 381  |\\n| Clone time   | 22s  | 52s   | 25s  |\\n| no. of files | 24995| 69447 | 5907 |\\n\\n## Security and governance won\'t allow us to take our code off z/OS.\\n\\nChoosing to adopt industry-standard tools to gain the advantages in working practices and \\ncommonality of skills has to include securely managing source code outside of its traditional\\nhome of z/OS data sets.\\n\\nThere are legitimate and important concerns about securing access to the source code, but there are plenty\\nof approaches which deal with them.\\n\\nSecuring employee IT access via laptops or workstations should be standard practice.\\nIf you don\'t want the cloned repos to reside on those developer workstations then \\nconsider the use of cloud-hosted IDEs which keep all the files within a secure server \\nenvironment, only presenting the user experience locally to the developer.\\n\\nGit service providers will allow you to control who can access the repository\\nwith varying levels of permissions. Github\'s permissions for each of the pre-defined organisation roles are described [here](https://docs.github.com/en/organizations/managing-user-access-to-your-organizations-repositories/managing-repository-roles/repository-roles-for-an-organization#permissions-for-each-role).\\n\\nBeyond access to the code via cloning and the ease with which local branches can\\nbe created and changes committed to them, the absolute safeguard against unauthorised\\nchange progressing is the Pull Request and Merge process. There are at least two levels\\nof safeguarding here too:\\n\\n- **Write-access to the repo** - without that, then it is impossible to publish a branch \\n  to the server copy of the repo.\\n  - There is a workflow based on tightly restricting write access to an entire repository\\n    which you might at first think is going to make contribution almost impossible, but it\\n    relies on contributors *forking* the repo to make their contributions and then using a \\n    Pull Request between their fork and the original repo. So again, the safeguard is the\\n    same approval process and control of write access to the original repository at the Pull Request level. Your git\\n    server should allow you to prevent forking.\\n- **Protected branches within the repo** - any branch can be subject to restrictions on who\\n  is permitted to commit to it. That restricts who can perform any merges from other branches\\n  as well as directly making new commits. Typically any integration branch would be protected, \\n  which ensures only authorised people can merge an approved Pull Request.\\n\\n## Wrapping up\\n\\nAt the risk of repeating myself, don\'t worry about the size of your repos, they will be fine.\\n\\nThat\'s \'fine\' as in git has evolved and been designed to be extremely scalable, there \\nare features and ways of doing things which you might need to learn and adopt and,\\nperhaps your code base isn\'t actually unusually large so you can see proof that it will be fine.\\n\\nOr, tell me why I\'m wrong! --\x3e"},{"id":"2023-12-27-transform-rbc","metadata":{"permalink":"/z-devops-acceleration-program/news/2023-12-27-transform-rbc","source":"@site/news/2023-12-27-devops-transform-with-git-at-rbc.md","title":"A standardized DevOps pipeline at RBC improves the developer experience and drives global business value","description":"In 2019, Royal Bank of Canada (RBC) began a project to automate a standardized DevOps pipeline that had been manual and leveraged an older library management system","date":"2023-12-27T00:00:00.000Z","formattedDate":"December 27, 2023","tags":[{"label":"transformation","permalink":"/z-devops-acceleration-program/news/tags/transformation"},{"label":"adoption","permalink":"/z-devops-acceleration-program/news/tags/adoption"}],"hasTruncateMarker":false,"authors":[{"name":"Sherri Hanna"}],"frontMatter":{"title":"A standardized DevOps pipeline at RBC improves the developer experience and drives global business value","slug":"2023-12-27-transform-rbc","authors":[{"name":"Sherri Hanna"}],"tags":["transformation","adoption"],"hide_table_of_contents":false,"hide_reading_time":true},"unlisted":false,"prevItem":{"title":"Worried about cloning your large Z code Git repo?","permalink":"/z-devops-acceleration-program/news/2024-03-08-cloning-large-z-repos"},"nextItem":{"title":"Welcome to the IBM Z DevOps Acceleration Program News","permalink":"/z-devops-acceleration-program/news/2023-11-28-opening-post"}},"content":"In 2019, Royal Bank of Canada (RBC) began a project to automate a standardized DevOps pipeline that had been manual and leveraged an older library management system\\nfor source code management. Then, as happened to all, the pandemic came along and while it extended the project, it also gave RBC time to\\ndevelop and implement a homegrown application on-boarding tool they named Helios to interact with and drive standardization\\nwith GitHub [read more...](https://community.ibm.com/community/user/ibmz-and-linuxone/blogs/sherri-hanna1/2023/12/27/rbc-standard-devops-pipeline?CommunityKey=f36c1ced-7e79-43cd-897c-e798acfef4a4)"},{"id":"2023-11-28-opening-post","metadata":{"permalink":"/z-devops-acceleration-program/news/2023-11-28-opening-post","source":"@site/news/2023-11-28-opening-post.md","title":"Welcome to the IBM Z DevOps Acceleration Program News","description":"On the IBM Z DevOps Acceleration Program News page, you will find links to relevant news and blogs that showcase and explain the value of our Git workflow-based CI/CD solution.","date":"2023-11-28T00:00:00.000Z","formattedDate":"November 28, 2023","tags":[],"readingTime":0.21,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Welcome to the IBM Z DevOps Acceleration Program News","slug":"2023-11-28-opening-post","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"A standardized DevOps pipeline at RBC improves the developer experience and drives global business value","permalink":"/z-devops-acceleration-program/news/2023-12-27-transform-rbc"},"nextItem":{"title":"A \\"no-baggage\\" approach to new Mainframe development practices","permalink":"/z-devops-acceleration-program/news/2023-11-22-no-baggage"}},"content":"On the [IBM Z DevOps Acceleration Program News page](https://ibm.github.io/z-devops-acceleration-program/news), you will find links to relevant news and blogs that showcase and explain the value of our Git workflow-based CI/CD solution.\\n\\nBookmark this page as we update it regularly with new content: https://ibm.github.io/z-devops-acceleration-program/news"},{"id":"2023-11-22-no-baggage","metadata":{"permalink":"/z-devops-acceleration-program/news/2023-11-22-no-baggage","source":"@site/news/2023-11-22-no-baggage.md","title":"A \\"no-baggage\\" approach to new Mainframe development practices","description":"Transitioning a whole delivery process for applications to a new set of tools and practices can represent a real challenge if the attempt is to reproduce design from the past and keep things as they were as close as possible. A \\"no-baggage\\" strategy means read more...","date":"2023-11-22T00:00:00.000Z","formattedDate":"November 22, 2023","tags":[{"label":"transformation","permalink":"/z-devops-acceleration-program/news/tags/transformation"},{"label":"adoption","permalink":"/z-devops-acceleration-program/news/tags/adoption"}],"hasTruncateMarker":false,"authors":[{"name":"Mathieu Dalbin"}],"frontMatter":{"title":"A \\"no-baggage\\" approach to new Mainframe development practices","slug":"2023-11-22-no-baggage","authors":[{"name":"Mathieu Dalbin"}],"tags":["transformation","adoption"],"hide_table_of_contents":false,"hide_reading_time":true},"unlisted":false,"prevItem":{"title":"Welcome to the IBM Z DevOps Acceleration Program News","permalink":"/z-devops-acceleration-program/news/2023-11-28-opening-post"},"nextItem":{"title":"Transformation of library manager SysProgs into DevOps Engineers","permalink":"/z-devops-acceleration-program/news/2023-06-09-sysprog-to-devops-eng"}},"content":"Transitioning a whole delivery process for applications to a new set of tools and practices can represent a real challenge if the attempt is to reproduce design from the past and keep things as they were as close as possible. A \\"no-baggage\\" strategy means [read more...](https://community.ibm.com/community/user/ibmz-and-linuxone/blogs/mathieu-dalbin/2024/02/09/a-no-baggage-approach-to-new-mainframe-development)"},{"id":"2023-06-09-sysprog-to-devops-eng","metadata":{"permalink":"/z-devops-acceleration-program/news/2023-06-09-sysprog-to-devops-eng","source":"@site/news/2023-06-09-sysprog-to-devops-eng.md","title":"Transformation of library manager SysProgs into DevOps Engineers","description":"Delivering software and services at the speed that the market demands, requires IT teams to work together, and iterate rapidly. To be responsive to business demands, enterprise development teams need to read more...","date":"2023-06-09T00:00:00.000Z","formattedDate":"June 9, 2023","tags":[{"label":"migration specialist","permalink":"/z-devops-acceleration-program/news/tags/migration-specialist"},{"label":"build specialist","permalink":"/z-devops-acceleration-program/news/tags/build-specialist"}],"hasTruncateMarker":false,"authors":[{"name":"Senthil Nathan"}],"frontMatter":{"title":"Transformation of library manager SysProgs into DevOps Engineers","slug":"2023-06-09-sysprog-to-devops-eng","authors":[{"name":"Senthil Nathan"}],"tags":["migration specialist","build specialist"],"hide_table_of_contents":false,"hide_reading_time":true},"unlisted":false,"prevItem":{"title":"A \\"no-baggage\\" approach to new Mainframe development practices","permalink":"/z-devops-acceleration-program/news/2023-11-22-no-baggage"},"nextItem":{"title":"Why Git and why ONLY Git?","permalink":"/z-devops-acceleration-program/news/2023-02-22-why-git"}},"content":"Delivering software and services at the speed that the market demands, requires IT teams to work together, and iterate rapidly. To be responsive to business demands, enterprise development teams need to [read more...](https://community.ibm.com/community/user/ibmz-and-linuxone/blogs/senthil-nathan/2023/07/06/library-manager-sysprogs-into-devops-engs)"},{"id":"2023-02-22-why-git","metadata":{"permalink":"/z-devops-acceleration-program/news/2023-02-22-why-git","source":"@site/news/2023-02-22-why-git.md","title":"Why Git and why ONLY Git?","description":"Git has been accepted as the primary Source Control Management (SCM) in the enterprise, open-source & startup world. It is a typical \u201cwinner takes all\u201d scenario, where very little is left to debate read more...","date":"2023-02-22T00:00:00.000Z","formattedDate":"February 22, 2023","tags":[{"label":"Git","permalink":"/z-devops-acceleration-program/news/tags/git"}],"hasTruncateMarker":false,"authors":[{"name":"Senthil Nathan"}],"frontMatter":{"title":"Why Git and why ONLY Git?","slug":"2023-02-22-why-git","authors":[{"name":"Senthil Nathan"}],"tags":["Git"],"hide_table_of_contents":false,"hide_reading_time":true},"unlisted":false,"prevItem":{"title":"Transformation of library manager SysProgs into DevOps Engineers","permalink":"/z-devops-acceleration-program/news/2023-06-09-sysprog-to-devops-eng"}},"content":"Git has been accepted as the primary Source Control Management (SCM) in the enterprise, open-source & startup world. It is a typical \u201cwinner takes all\u201d scenario, where very little is left to debate [read more...](https://community.ibm.com/community/user/ibmz-and-linuxone/blogs/senthil-nathan/2023/02/21/why-only-git)"}]}')}}]);